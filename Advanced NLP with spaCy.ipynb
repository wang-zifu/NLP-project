{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP with spaCy (online course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English library class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Finding words, phrases, names and concepts\n",
    "\n",
    "Basic text prcoessing, data structures, statistical models and how to predict linguistic features in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more\n",
      "True False False\n",
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object (contains processing pipeline)\n",
    "nlp = English()\n",
    "\n",
    "# Document object (created when nlp called on a string - text is tokenized)\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Tokens in a document (i.e. words or punctuation)\n",
    "# Token objects have attributes to access information, e.g. .text\n",
    "token = doc[3]\n",
    "print(token.text)\n",
    "    \n",
    "# Span object is a slice of the Document (i.e. multiple tokens)\n",
    "# Only used to view Doc, doesn't contain any data itself\n",
    "span = doc[1:3]\n",
    "\n",
    "# Lexical attributes (returns boolean) (refer to vocab, and don't depend on token's context)\n",
    "print(token.is_alpha, token.is_punct, token.like_num)\n",
    "\n",
    "# Example search for percentages in text\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical models\n",
    "\n",
    "Statistical models allow spaCy to make predictions in context  \n",
    "e.g.part-of-speech tags, syntactic dependencies, named entities  \n",
    "Models trained on labelled data  \n",
    "\n",
    "spaCy provides pretrained model packages  \n",
    "e.g. en_core_web_sm is a small english model trained on web text  \n",
    "Package provides binary weights, vocabulary, meta information (langauge + how to configure processing pipeline)\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an integer ID value.\n",
    "\n",
    ".pos_ = predicted part-of-speech tag (i.e. word types in context)  \n",
    ".dep_ = predicted (syntactic) dependency level  \n",
    ".head = syntactic head token (i.e. parent token this word is attached to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She     PRON    nsubj   ate     \n",
      "ate     VERB    ROOT    ate     \n",
      "the     DET     det     pizza   \n",
      "pizza   NOUN    dobj    ate     \n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# Loads the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    # Print text and text predictions\n",
    "    #print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    print(f\"{token.text:<8}{token.pos_:<8}{token.dep_:<8}{token.head.text:<8}\")\n",
    "    \n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted (named) entities (e.g. a person, organization, country)\n",
    "# .ents returns an iterator of Span objects\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Use explain() helper function to get definitions for common tags and labels\n",
    "print(spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based matching\n",
    "\n",
    "Why not just use regular expressions?  \n",
    "The matcher works with Doc and Token objects, instead of just strings  \n",
    "You can match on tokens and token attributes  \n",
    "You can write rules that use the model's predictions\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.  \n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun.\n",
    "\n",
    "Four \"OP\" values:  \n",
    "{\"OP\": \"!\"}\tNegation: match 0 times  \n",
    "{\"OP\": \"?\"}\tOptional: match 0 or 1 times  \n",
    "{\"OP\": \"+\"}\tMatch 1 or more times  \n",
    "{\"OP\": \"*\"}\tMatch 0 or more times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "# Match exact token texts\n",
    "pattern1 = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "# Match lexical attributes\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "# Match any token attributes\n",
    "pattern3 = [{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]     # e.g. buying milk, bought flowers\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "    \n",
    "# Pattern for matching lexical attributes\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# Pattern for matching other token attributes\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Pattern showing use of operators and quantifiers\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "Extract specific information from large volumes of text. How to make the most of spaCy's data structures. How to effectively combine statistical and rule-based approaches of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures 1 - Vocab, Lexemes and StringStore\n",
    "\n",
    "Vocab: stores data shared across multiple documents  \n",
    "To save memory, spaCy encodes all strings to hash values  \n",
    "Strings are only stored once in the StringStore via nlp.vocab.strings  \n",
    "String store: lookup table in both directions  \n",
    "\n",
    "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab.\n",
    "\n",
    "The doc also exposes the vocab and strings  \n",
    "i.e. `doc.vocab.strings['coffee']`\n",
    "\n",
    "Lexemes are context-independent entries in the vocabulary.  \n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.  \n",
    "Lexemes expose attributes, just like tokens.  \n",
    "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.  \n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.\n",
    "\n",
    "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.  \n",
    "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n",
      "Lexical attributes: coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "\n",
    "# Raises an error if we haven't seen the string before\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])\n",
    "\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print('Lexical attributes:', lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures 2 - Doc, Span and Token\n",
    "\n",
    "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.  \n",
    "The spaces are a list of boolean values indicating whether the word is followed by a space.  \n",
    "The Doc class takes three arguments: the shared vocab, the words and the spaces.\n",
    "\n",
    "A Span is a slice of a doc consisting of one or more tokens.  \n",
    "The Span takes at least three arguments: the doc it refers to, and the start and end index of the span.\n",
    "\n",
    "To create a Span manually, we can also import the class from spacy.tokens. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.  \n",
    "The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
    "\n",
    "Best practice tips:\n",
    "- The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "- If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "- To keep things consistent, try to use built-in token attributes wherever possible. For example, token.i for the token index.\n",
    "- Also, don't forget to always pass in the shared vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "# Example to analyse text and collect all proper nouns that are followed by a verb.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and semantic similarity\n",
    "\n",
    "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.  \n",
    "The Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.  \n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included. For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\".  \n",
    "You can also use the similarity methods to compare different types of objects.\n",
    "\n",
    "**Word vectors:**  \n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.  \n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.  \n",
    "Vectors can be added to spaCy's statistical models.  \n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.  \n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.  \n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words.\n",
    "\n",
    "**Similarity depends on the application context:**  \n",
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.  \n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.  \n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n",
      "0.6199091710787739\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "print(span.similarity(doc))\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)\n",
    "# The result is a 300-dimensional vector of the word \"banana\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models and rules\n",
    "\n",
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.  \n",
    "For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.  \n",
    "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger.\n",
    "\n",
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.  \n",
    "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher.\n",
    "\n",
    "**Efficient phrase matching:**  \n",
    "The phrase matcher is another helpful tool to find sequences of words in your data.  \n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.  \n",
    "It takes Doc objects as patterns.  \n",
    "It's also really fast.  \n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text.  \n",
    "The phrase matcher can be imported from spacy.matcher and follows the same API as the regular matcher.  \n",
    "Instead of a list of dictionaries, we pass in a Doc object as the pattern.  \n",
    "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "# Adding statistical predictions to rule based matching:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"DOG\", None, [{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "# Phrase matching example\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "# Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. \n",
    "\n",
    "COUNTRIES = [\"France\", \"Czech Republic\", \"Slovakia\"]\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Processing pipelines\n",
    "\n",
    "What goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipelines\n",
    "\n",
    "Processing pipelines: a series of functions applied to a doc to add attributes like part-of-speech tags, dependency labels or named entities.\n",
    "\n",
    "**What does the nlp object actually do?**  \n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it.\n",
    "\n",
    "**Built in pipeline components:**  \n",
    "Name - Description - Creates\n",
    "\n",
    "tagger - Part-of-speech tagger\t- Token.tag, Token.pos  \n",
    "parser - Dependency parser\t- Token.dep, Token.head, Doc.sents, Doc.noun_chunks  \n",
    "ner\t- Named entity recognizer - Doc.ents, Token.ent_iob, Token.ent_type  \n",
    "textcat\t- Text classifier - Doc.cats\n",
    "\n",
    "All models you can load into spaCy include several files and a meta.json.  \n",
    "The meta defines things like the language and pipeline. This tells spaCy which components to instantiate.  \n",
    "The built-in components that make predictions also need binary data. The data is included in the model package and loaded into the component when you load the model.\n",
    "\n",
    "`nlp.pipe_names`: list of pipeline component names  \n",
    "`nlp.pipeline`: list of (name, component) tuples\n",
    "\n",
    "The component functions are the functions applied to the doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x000002154FAB5688>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x000002154F7C13A8>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x000002154F99D4C8>)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline components\n",
    "\n",
    "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.  \n",
    "Custom components are executed automatically when you call the nlp object on a text.  \n",
    "They're especially useful for adding your own custom metadata to documents and tokens.  \n",
    "You can also use them to update built-in attributes, like the named entity spans.\n",
    "\n",
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.  \n",
    "Components can be added to the pipeline using the nlp.add_pipe method. The method takes at least one argument: the component function.\n",
    "\n",
    "To specify where to add the component in the pipeline, you can use the following keyword arguments:  \n",
    "Setting last to True will add the component last in the pipeline. This is the default behavior.  \n",
    "Setting first to True will add the component first in the pipeline, right after the tokenizer.  \n",
    "The before and after arguments let you define the name of an existing component to add the new component before or after. For example, before=\"ner\" will add it before the named entity recognizer.  \n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# Example custom component to add animals in the text to doc.ents\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attributes\n",
    "\n",
    "Custom attributes let you add any metadata to docs, tokens and spans. The data can be added once, or it can be computed dynamically.  \n",
    "Custom attributes are available via the ._ (dot underscore) property. This makes it clear that they were added by the user, and not built into spaCy, like token.text.  \n",
    "Attributes need to be registered on the global Doc, Token and Span classes you can import from spacy.tokens.  \n",
    "To register a custom attribute on the Doc, Token and Span, you can use the set_extension method.  \n",
    "The first argument is the attribute name. Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten.\n",
    "\n",
    "```\n",
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "\n",
    "There are three types of extensions: attribute extensions, property extensions and method extensions.\n",
    "\n",
    "**Atribute extensions:** \n",
    "Attribute extensions set a default value that can be overwritten.  \n",
    "For example, a custom is_color attribute on the token that defaults to False.  \n",
    "On individual tokens, its value can be changed by overwriting it – in this case, True for the token \"blue\".\n",
    "\n",
    "**Property extensions:**  \n",
    "Property extensions work like properties in Python: they can define a getter function and an optional setter.  \n",
    "The getter function is only called when you retrieve the attribute. This lets you compute the value dynamically, and even take other custom attributes into account.  \n",
    "Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors.  \n",
    "We can then provide the function via the getter keyword argument when we register the extension.  \n",
    "The token \"blue\" now returns True for `._.is_color.`\n",
    "\n",
    "If you want to set extension attributes on a span, you almost always want to use a property extension with a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values.  \n",
    "In this example, the get_has_color function takes the span and returns whether the text of any of the tokens is in the list of colors.  \n",
    "After we've processed the doc, we can check different slices of the doc and the custom `._.has_color` property returns whether the span contains a color token or not.\n",
    "\n",
    "**Method extensions:**  \n",
    "Method extensions make the extension attribute a callable method.\n",
    "\n",
    "You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting.\n",
    "\n",
    "In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, token_text.\n",
    "\n",
    "Here, the custom `._.has_token` method returns True for the word \"blue\" and False for the word \"cloud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "True - sky is blue\n",
      "False - The sky\n",
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension(\"title\", default=None, force=True)\n",
    "Token.set_extension(\"is_color\", default=False, force=True)\n",
    "Span.set_extension(\"has_color\", default=False, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True)\n",
    "\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color, force=True)\n",
    "\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)\n",
    "\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and performance\n",
    "\n",
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the `nlp.pipe` method can speed this up significantly.  \n",
    "It processes the texts as a stream and yields Doc objects.  \n",
    "It is much faster than just calling nlp on each text, because it batches up the texts.  \n",
    "`nlp.pipe` is a generator that yields Doc objects, so in order to get a list of docs, remember to call the list method around it.\n",
    "\n",
    "`nlp.pipe` also supports passing in tuples of text / context if you set as_tuples to True.  \n",
    "The method will then yield doc / context tuples.  \n",
    "This is useful for passing in additional metadata, like an ID associated with the text, or a page number.\n",
    "\n",
    "You can even add the context metadata to custom attributes.  \n",
    "In this example, we're registering two extensions, id and page_number, which default to None.  \n",
    "After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata.\n",
    "\n",
    "Another common scenario: Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text.  \n",
    "Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need.  \n",
    "If you only need a tokenized Doc object, you can use the nlp.make_doc method instead, which takes a text and returns a doc.\n",
    "This is also how spaCy does it behind the scenes: `nlp.make_doc` turns the text into a doc before the pipeline components are called.\n",
    "\n",
    "spaCy also allows you to temporarily disable pipeline components using the nlp.disable_pipes context manager.  \n",
    "It takes a variable number of arguments, the string names of the pipeline components to disable. For example, if you only want to use the entity recognizer to process a document, you can temporarily disable the tagger and parser.  \n",
    "After the with block, the disabled pipeline components are automatically restored.  \n",
    "In the with block, spaCy will only run the remaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'id' already exists on Doc. To overwrite the existing extension, set `force=True` on `Doc.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d37ea1af8e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mDoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"page_number\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E090] Extension 'id' already exists on Doc. To overwrite the existing extension, set `force=True` on `Doc.set_extension`."
     ]
    }
   ],
   "source": [
    "# Efficient way to process lots of texts\n",
    "LOTS_OF_TEXTS = ['text1', 'text2']\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "    \n",
    "# Passing in context example\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]\n",
    "    print(doc.text, context[\"page_number\"])\n",
    "    \n",
    "# Only tokenizes the text\n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp('text')\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Training a neural network model\n",
    "\n",
    "Update spaCy's statistical models to customize them for your use case – for example, to predict a new entity type in online comments. You'll write your own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make your custom NLP projects more successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and updating models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
