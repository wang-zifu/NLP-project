{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP with spaCy (online course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English library class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Finding words, phrases, names and concepts\n",
    "\n",
    "Basic text prcoessing, data structures, statistical models and how to predict linguistic features in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more\n",
      "True False False\n",
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object (contains processing pipeline)\n",
    "nlp = English()\n",
    "\n",
    "# Document object (created when nlp called on a string - text is tokenized)\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Tokens in a document (i.e. words or punctuation)\n",
    "# Token objects have attributes to access information, e.g. .text\n",
    "token = doc[3]\n",
    "print(token.text)\n",
    "    \n",
    "# Span object is a slice of the Document (i.e. multiple tokens)\n",
    "# Only used to view Doc, doesn't contain any data itself\n",
    "span = doc[1:3]\n",
    "\n",
    "# Lexical attributes (returns boolean) (refer to vocab, and don't depend on token's context)\n",
    "print(token.is_alpha, token.is_punct, token.like_num)\n",
    "\n",
    "# Example search for percentages in text\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical models\n",
    "\n",
    "Statistical models allow spaCy to make predictions in context  \n",
    "e.g.part-of-speech tags, syntactic dependencies, named entities  \n",
    "Models trained on labelled data  \n",
    "\n",
    "spaCy provides pretrained model packages  \n",
    "e.g. en_core_web_sm is a small english model trained on web text  \n",
    "Package provides binary weights, vocabulary, meta information (langauge + how to configure processing pipeline)\n",
    "\n",
    "In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an integer ID value.\n",
    "\n",
    ".pos_ = predicted part-of-speech tag (i.e. word types in context)  \n",
    ".dep_ = predicted (syntactic) dependency level  \n",
    ".head = syntactic head token (i.e. parent token this word is attached to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She     PRON    nsubj   ate     \n",
      "ate     VERB    ROOT    ate     \n",
      "the     DET     det     pizza   \n",
      "pizza   NOUN    dobj    ate     \n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# Loads the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    # Print text and text predictions\n",
    "    #print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    print(f\"{token.text:<8}{token.pos_:<8}{token.dep_:<8}{token.head.text:<8}\")\n",
    "    \n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted (named) entities (e.g. a person, organization, country)\n",
    "# .ents returns an iterator of Span objects\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Use explain() helper function to get definitions for common tags and labels\n",
    "print(spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based matching\n",
    "\n",
    "Why not just use regular expressions?  \n",
    "The matcher works with Doc and Token objects, instead of just strings  \n",
    "You can match on tokens and token attributes  \n",
    "You can write rules that use the model's predictions\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.  \n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun.\n",
    "\n",
    "Four \"OP\" values:  \n",
    "{\"OP\": \"!\"}\tNegation: match 0 times  \n",
    "{\"OP\": \"?\"}\tOptional: match 0 or 1 times  \n",
    "{\"OP\": \"+\"}\tMatch 1 or more times  \n",
    "{\"OP\": \"*\"}\tMatch 0 or more times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "# Match exact token texts\n",
    "pattern1 = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "# Match lexical attributes\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "# Match any token attributes\n",
    "pattern3 = [{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]     # e.g. buying milk, bought flowers\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "    \n",
    "# Pattern for matching lexical attributes\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# Pattern for matching other token attributes\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Pattern showing use of operators and quantifiers\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "# Example text with above pattern\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "Extract specific information from large volumes of text. How to make the most of spaCy's data structures. How to effectively combine statistical and rule-based approaches of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures 1 - Vocab, Lexemes and StringStore\n",
    "\n",
    "Vocab: stores data shared across multiple documents  \n",
    "To save memory, spaCy encodes all strings to hash values  \n",
    "Strings are only stored once in the StringStore via nlp.vocab.strings  \n",
    "String store: lookup table in both directions  \n",
    "\n",
    "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab.\n",
    "\n",
    "The doc also exposes the vocab and strings  \n",
    "i.e. `doc.vocab.strings['coffee']`\n",
    "\n",
    "Lexemes are context-independent entries in the vocabulary.  \n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.  \n",
    "Lexemes expose attributes, just like tokens.  \n",
    "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.  \n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.\n",
    "\n",
    "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.  \n",
    "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n",
      "Lexical attributes: coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "\n",
    "# Raises an error if we haven't seen the string before\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])\n",
    "\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print('Lexical attributes:', lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures 2 - Doc, Span and Token\n",
    "\n",
    "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.  \n",
    "The spaces are a list of boolean values indicating whether the word is followed by a space.  \n",
    "The Doc class takes three arguments: the shared vocab, the words and the spaces.\n",
    "\n",
    "A Span is a slice of a doc consisting of one or more tokens.  \n",
    "The Span takes at least three arguments: the doc it refers to, and the start and end index of the span.\n",
    "\n",
    "To create a Span manually, we can also import the class from spacy.tokens. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.  \n",
    "The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
    "\n",
    "Best practice tips:\n",
    "- The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "- If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "- To keep things consistent, try to use built-in token attributes wherever possible. For example, token.i for the token index.\n",
    "- Also, don't forget to always pass in the shared vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "# Example to analyse text and collect all proper nouns that are followed by a verb.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and semantic similarity\n",
    "\n",
    "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.  \n",
    "The Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.  \n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included. For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\".  \n",
    "You can also use the similarity methods to compare different types of objects.\n",
    "\n",
    "**Word vectors:**  \n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.  \n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.  \n",
    "Vectors can be added to spaCy's statistical models.  \n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.  \n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.  \n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words.\n",
    "\n",
    "**Similarity depends on the application context:**  \n",
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.  \n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.  \n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n",
      "0.6199091710787739\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "print(span.similarity(doc))\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)\n",
    "# The result is a 300-dimensional vector of the word \"banana\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models and rules\n",
    "\n",
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.  \n",
    "For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.  \n",
    "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger.\n",
    "\n",
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.  \n",
    "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher.\n",
    "\n",
    "**Efficient phrase matching:**  \n",
    "The phrase matcher is another helpful tool to find sequences of words in your data.  \n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.  \n",
    "It takes Doc objects as patterns.  \n",
    "It's also really fast.  \n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text.  \n",
    "The phrase matcher can be imported from spacy.matcher and follows the same API as the regular matcher.  \n",
    "Instead of a list of dictionaries, we pass in a Doc object as the pattern.  \n",
    "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "# Adding statistical predictions to rule based matching:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"DOG\", None, [{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "# Phrase matching example\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "# Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. \n",
    "\n",
    "COUNTRIES = [\"France\", \"Czech Republic\", \"Slovakia\"]\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Processing pipelines\n",
    "\n",
    "What goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
